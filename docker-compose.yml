version: '3.4'

services:
  llm-backend:
    image: jmformenti/llama-cpp-python-cuda:v0.2.26
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: on-failure
    ports:
      - "8000:8000"
    env_file:
      - llm.env
    volumes:
      - ./model:/var/model

  chat-ui-db:
    image: ghcr.io/huggingface/chat-ui-db:latest
    # patch to prevent errors when starting a new chat (the real problem is that llama-cpp-python does not 
    # currently support parallel requests, https://github.com/abetlen/llama-cpp-python/pull/951)
    command: bash -c "sed -i 's/let title = \"\";$/let title = \"No Name\";/' /app/src/routes/conversation/+server.ts && /app/entrypoint.sh"
    ports:
      - "3000:3000"
    volumes:
      - ./chatui.env:/app/.env.local

