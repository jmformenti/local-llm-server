version: '3.4'

x-base-llm-backend: &base_llm_backend
    restart: on-failure
    ports:
      - "8000:8000"
    env_file:
      - llm.env
    volumes:
      - ./model:/var/model
    networks:
      default:
        aliases:
          - llm-backend

services:
  llm-backend-gpu:
    image: jmformenti/llama-cpp-python-cuda:v0.2.26
    <<: *base_llm_backend
    profiles:
      - gpu
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  llm-backend-cpu:
    image: ghcr.io/abetlen/llama-cpp-python:v0.2.26
    <<: *base_llm_backend
    profiles:
      - only-cpu

  chat-ui-db:
    image: ghcr.io/huggingface/chat-ui-db:latest
    # patch to prevent errors when starting a new chat (the real problem is that llama-cpp-python does not 
    # currently support parallel requests, https://github.com/abetlen/llama-cpp-python/pull/951)
    command: bash -c "sed -i 's/let title = \"\";$/let title = \"No Name\";/' /app/src/routes/conversation/+server.ts && /app/entrypoint.sh"
    ports:
      - "3000:3000"
    volumes:
      - ./chatui.env:/app/.env.local

